{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "# projects = ['abinit', 'libmesh', 'lammps', 'mdanalysis']\n",
    "projects = ['libmesh']\n",
    "for sp in projects:\n",
    "    s = \"/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/{}/\".format(sp)\n",
    "    path = s\n",
    "    files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    all_files.extend(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.3.2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.6.3_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v1.3.0-rc2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.3.3_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.2-final_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.4-rc2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_revert_projection4_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.7.2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.4.2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.3.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.6.1_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.7.1_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.4.1_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.1-final_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_revert_projection2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.6.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v1.0.0-rc2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.3_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.3.4_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.4.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.5.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_revert_projection3_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.7.0.4_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.3_commits_clean(1).csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_revert_projection_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.5-rc2_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v1.1.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v1.2.0_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v0.9.2.1_commits_clean.csv', '/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/libmesh/libmesh_v1.2.1_commits_clean.csv']\n"
     ]
    }
   ],
   "source": [
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = \"/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/auto/abinit_concat.csv\"\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pd.read_csv(fname, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "raw_df = concatenated_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy\n",
      "0.0    7913\n",
      "1.0     835\n",
      "Name: hash, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "buggies = raw_df.groupby(\"buggy\")[\"hash\"].count()\n",
    "print(buggies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_n = 1000\n",
    "# drop_indices = np.random.choice(raw_df.index, remove_n, replace=False)\n",
    "# new_df = raw_df.drop(drop_indices)\n",
    "# b = new_df.groupby(\"buggy\")[\"hash\"].count()\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash        object\n",
      "time        object\n",
      "message     object\n",
      "buggy      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(raw_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8748, 4)\n",
      "(8745, 4)\n"
     ]
    }
   ],
   "source": [
    "# inft = []\n",
    "# for el in y:\n",
    "#     if not np.isfinite(el):\n",
    "#         inft.append(el)\n",
    "# print(el)\n",
    "print(raw_df.shape)\n",
    "raw_df = raw_df.dropna()\n",
    "print(raw_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = raw_df['buggy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    [a, number, of, minor, change, .]\n",
      "1                              [cv, ignores, for, sgi]\n",
      "2    [this, commit, wa, manufactured, by, cvs2svn, ...\n",
      "3    [can, now, write, a, file, in, parallel, --, t...\n",
      "4              [updated, compatible, to, new, doxygen]\n",
      "Name: tknz_msg, dtype: object\n"
     ]
    }
   ],
   "source": [
    "raw_df['tknz_msg'] = raw_df['message'].apply(wordpunct_tokenize).apply(lambda tkns: [lemmatizer.lemmatize(w.lower()) for w in tkns])\n",
    "print(raw_df['tknz_msg'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              [number, minor, change]\n",
      "1                                       [ignores, sgi]\n",
      "2    [commit, manufactured, cvs2svn, create, tag, l...\n",
      "3    [write, file, parallel, format, identical, exi...\n",
      "4                  [updated, compatible, new, doxygen]\n",
      "Name: msg, dtype: object\n"
     ]
    }
   ],
   "source": [
    "raw_df['msg'] = raw_df['tknz_msg']\\\n",
    "    .apply(lambda tkns: \\\n",
    "           list(filter(\\\n",
    "                       lambda word: word not in stopset \\\n",
    "                       and word not in string.punctuation\\\n",
    "                       and len(word) > 2\\\n",
    "                       , tkns)))\n",
    "print(raw_df['msg'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = raw_df['msg'].apply(pd.Series).stack().drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(words))\n",
    "# print(words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['msg_str'] = raw_df['msg'].apply(lambda tkns: ' '.join(tkns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_features = min(1000, len(words))\n",
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(raw_df['msg_str'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features)\n",
    "X_tf = tf_vectorizer.fit_transform(raw_df['msg_str'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf vectorized:  (8745, 1000)\n",
      "tf vectorized:  (8745, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"tf-idf vectorized: \", X_tfidf.shape)\n",
    "print(\"tf vectorized: \", X_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 100\n",
    "num_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=num_iter, learning_method='online', learning_offset=50.,random_state=9, evaluate_every=100).fit(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8745, 100)\n"
     ]
    }
   ],
   "source": [
    "lda_x = lda.transform(X_tf)\n",
    "print(lda_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [\"bug\", \"fix\", \"wrong\", \"error\", \"fail\", \"problem\", \"patch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(\"Topic %d:\" % (topic_idx))\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "#         print(\" \".join(top_features))\n",
    "        if any(bug_word in top_features for bug_word in key_words):\n",
    "            print(\"Topic %d:\" % (topic_idx))\n",
    "            print(\" \".join(top_features))\n",
    "            topic_indices.append(topic_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 22:\n",
      "fix disable option configure typo amr vtk command dof_id_type fparser\n",
      "Topic 33:\n",
      "documentation updated fix mpi disable minor reason really outdated several\n",
      "Topic 49:\n",
      "function code one problem matrix small initial script solve shape\n",
      "Topic 58:\n",
      "file error include header output patch ignore exodus print svn\n",
      "Topic 63:\n",
      "fix api change include time every functor paste moving correctly\n",
      "Topic 69:\n",
      "file failure boyce testing unsteadysolver hilbert send reading bug meshcommunication\n",
      "Topic 74:\n",
      "fixed bug assert switch statement longer eigen thread memory boundary_ids\n",
      "Topic 90:\n",
      "use fix complex std regression lagrange iterator access around flagging\n",
      "Topic 98:\n",
      "hex8 directly tensor interpolation reset assert reduced_basis able wrong short\n",
      "[22, 33, 49, 58, 63, 69, 74, 90, 98]\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "# display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "print(topic_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top_map = {}\n",
    "def get_topic_top_words(model, feature_names):\n",
    "    if str(model) in model_top_map:\n",
    "        return model_top_map[str(model)]\n",
    "    topic_top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "            topic_top_words.append(top_words)\n",
    "    model_top_map[str(model)] = topic_top_words\n",
    "    return topic_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(words, no_top_words, model, feature_names):\n",
    "    topic_ranks = []\n",
    "    topic_top_words = get_topic_top_words(model, feature_names)\n",
    "    for top_words in topic_top_words:\n",
    "        topic_freq = 0\n",
    "        for w in words:\n",
    "            if w in top_words:\n",
    "                topic_freq += 1\n",
    "        topic_ranks.append(topic_freq)\n",
    "    buggy_topic = 0\n",
    "    max_val = max(topic_ranks)\n",
    "    idx = topic_ranks.index(max_val)\n",
    "    if idx in topic_indices:\n",
    "        buggy_topic = 1\n",
    "    return max_val, idx, buggy_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['topic_freq'], raw_df['topic_id'], raw_df['buggy_topic'] = zip(*raw_df['msg'].apply(lambda tkns: get_top_topics(tkns, 20, lda, tf_feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  (22, 0.0) value:  373\n",
      "index:  (22, 1.0) value:  270\n",
      "index:  (33, 0.0) value:  111\n",
      "index:  (33, 1.0) value:  20\n",
      "index:  (49, 0.0) value:  82\n",
      "index:  (49, 1.0) value:  7\n",
      "index:  (58, 0.0) value:  194\n",
      "index:  (58, 1.0) value:  16\n",
      "index:  (63, 0.0) value:  57\n",
      "index:  (63, 1.0) value:  10\n",
      "index:  (69, 0.0) value:  11\n",
      "index:  (69, 1.0) value:  1\n",
      "index:  (74, 0.0) value:  86\n",
      "index:  (74, 1.0) value:  47\n",
      "index:  (90, 0.0) value:  36\n",
      "index:  (90, 1.0) value:  17\n",
      "index:  (98, 0.0) value:  3\n"
     ]
    }
   ],
   "source": [
    "tops_labels = raw_df.groupby(['topic_id','buggy']).size()\n",
    "for i, v in tops_labels.items():\n",
    "    if i[0] in topic_indices:\n",
    "        print('index: ', i, 'value: ', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "final_df = raw_df[raw_df['buggy'] == 1]\n",
    "if len(topic_indices) > 0:\n",
    "    for idx in topic_indices:\n",
    "        final_df = final_df.append(raw_df[raw_df['topic_id'] == idx])\n",
    "    test_df = raw_df[(raw_df['hash'].apply(lambda x: x not in final_df['hash'].values))]\n",
    "else:\n",
    "    final_df = raw_df\n",
    "# final_df = raw_df\n",
    "use_all_as_test = False\n",
    "if not test_df.empty and test_df.size > 0:\n",
    "    use_all_as_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8745, 10)\n",
      "(2176, 10)\n",
      "(6911, 10)\n"
     ]
    }
   ],
   "source": [
    "print(raw_df.shape)\n",
    "print(final_df.shape)\n",
    "if use_all_as_test:\n",
    "    print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_X_tf = tf_vectorizer.transform(final_df['msg_str'])\n",
    "# X = lda.transform(final_X_tf)\n",
    "X = tf_vectorizer.transform(final_df['msg_str'])\n",
    "y = final_df['buggy']\n",
    "if use_all_as_test:\n",
    "#     test_X_tf = tf_vectorizer.transform(test_df['msg_str'])\n",
    "#     test_X = lda.transform(test_X_tf)\n",
    "    test_X = tf_vectorizer.transform(test_df['msg_str'])\n",
    "    test_y = test_df['buggy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176, 1000)\n",
      "(2176,)\n",
      "(2176, 1001)\n",
      "(6911, 1001)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# X = hstack((X,np.array(final_df['topic_id'])[:,None]))\n",
    "# X = hstack((X,np.array(final_df['topic_freq'])[:,None]))\n",
    "X = hstack((X,np.array(final_df['buggy_topic'])[:,None]))\n",
    "print(X.shape)\n",
    "if use_all_as_test:\n",
    "#     test_X = hstack((test_X,np.array(test_df['topic_id'])[:,None]))\n",
    "#     test_X = hstack((test_X,np.array(test_df['topic_freq'])[:,None]))\n",
    "    test_X = hstack((test_X,np.array(test_df['buggy_topic'])[:,None]))\n",
    "    print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=9, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# positive in train set: 917 \n",
      "# negative in train set: 715\n",
      "# positive in test set: 306 \n",
      "# negative in test set: 238\n"
     ]
    }
   ],
   "source": [
    "print(\"# positive in train set: {}\".format(len(y_train[y_train == 1])),\n",
    "      \"\\n# negative in train set: {}\".format(len(y_train[y_train == 0])))\n",
    "print(\"# positive in test set: {}\".format(len(y_test[y_test == 1])),\n",
    "      \"\\n# negative in test set: {}\".format(len(y_test[y_test == 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=9,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = MultinomialNB()\n",
    "# clf = svm.LinearSVC(C=100, loss='hinge', random_state=9, max_iter=500000)\n",
    "clf = svm.SVC(C=100, kernel='linear', random_state=9)\n",
    "# clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([254, 294], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7455, 1001)\n"
     ]
    }
   ],
   "source": [
    "if use_all_as_test:\n",
    "    mod_X_test = vstack((test_X, X_test)).todense()\n",
    "    print(mod_X_test.shape)\n",
    "    pred_y = clf.predict(mod_X_test)\n",
    "#     pred_y = clf.predict(X_test)\n",
    "else:\n",
    "    pred_y = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred_y = clf.predict(X_test)\n",
    "# print(p_pred_y.shape)\n",
    "if use_all_as_test:\n",
    "    p_pred_y = np.append(p_pred_y, clf.predict(test_X))\n",
    "# print(p_pred_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7455,)\n"
     ]
    }
   ],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "if use_all_as_test:\n",
    "    mod_y_test = np.append(test_y, y_test)\n",
    "    print(mod_y_test.shape)\n",
    "#     mod_y_test = y_test\n",
    "else:\n",
    "    mod_y_test = y_test\n",
    "tn, fp, fn, tp = confusion_matrix(mod_y_test, p_pred_y).ravel()\n",
    "\n",
    "acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "prec = tp / (tp + fp)\n",
    "rec = tp / (tp + fn)\n",
    "f1 = 2 * prec * rec / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.13427230046948357\n",
      "precision 0.04459259259259259\n",
      "recall 0.9836601307189542\n",
      "f1 0.08531746031746032\n",
      "700 6449 5 301\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy\", acc)\n",
    "print(\"precision\", prec)\n",
    "print(\"recall\", rec)\n",
    "print(\"f1\", f1)\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "libmesh_fastread_fname = \"/Users/saurabh/workspace/fss/project/data/data-collection/labeled_commits/fastread/abinit_fast_labeled.csv\"\n",
    "libmesh_df = pd.read_csv(libmesh_fastread_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      "no               445\n",
      "undetermined    3791\n",
      "yes              676\n",
      "Name: hash, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "libmesh_df.columns = [\"hash\", \"abstract\", 'year', \"lnk\", \"label\", \"code\", \"time\"]\n",
    "codes = libmesh_df.groupby(\"code\")[\"hash\"].count()\n",
    "print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4912, 7)\n",
      "(8745, 10)\n",
      "hash         object\n",
      "abstract     object\n",
      "year          int64\n",
      "lnk         float64\n",
      "label       float64\n",
      "code         object\n",
      "time        float64\n",
      "dtype: object\n",
      "hash            object\n",
      "time            object\n",
      "message         object\n",
      "buggy          float64\n",
      "tknz_msg        object\n",
      "msg             object\n",
      "msg_str         object\n",
      "topic_freq       int64\n",
      "topic_id         int64\n",
      "buggy_topic      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "raw_df['hash'] = raw_df['hash'].astype(str)\n",
    "libmesh_df['hash'] = libmesh_df['hash'].astype(str)\n",
    "print(libmesh_df.shape)\n",
    "print(raw_df.shape)\n",
    "print(libmesh_df.dtypes)\n",
    "print(raw_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hash                                            msg_str  buggy_topic  buggy          code\n",
      "0  0.0  fixing another similar hermite projection erro...            0    1.0  undetermined\n",
      "1  0.0                       oops forgot define petsc_lib            0    0.0  undetermined\n",
      "2  0.0      updating configure use silent rule color test            1    0.0  undetermined\n",
      "3  0.0  reworked inffe inverse_map work infhex8 infpri...            0    0.0  undetermined\n",
      "4  0.0  merge pull request 1158 roystgnr adaptivity_ex...            0    0.0  undetermined\n",
      "(5, 16)\n",
      "  hash                                            msg_str  buggy_topic  buggy          code\n",
      "0  0.0  fixing another similar hermite projection erro...            0    1.0  undetermined\n",
      "1  0.0                       oops forgot define petsc_lib            0    0.0  undetermined\n",
      "2  0.0      updating configure use silent rule color test            1    0.0  undetermined\n",
      "3  0.0  reworked inffe inverse_map work infhex8 infpri...            0    0.0  undetermined\n",
      "4  0.0  merge pull request 1158 roystgnr adaptivity_ex...            0    0.0  undetermined\n",
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "libmesh_df_merged = pd.merge(raw_df, libmesh_df, how='inner', on=['hash'], suffixes=(\"_raw\", \"_libmesh\"))\n",
    "print(libmesh_df_merged[['hash', 'msg_str', 'buggy_topic', 'buggy', 'code']].head(10))\n",
    "print(libmesh_df_merged.shape)\n",
    "libmesh_df_merged_d = libmesh_df_merged[['hash', 'msg_str', 'buggy_topic', 'buggy', 'code']]\n",
    "print(libmesh_df_merged_d.head(10))\n",
    "print(libmesh_df_merged_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1001)\n",
      "(5, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_libmesh = tf_vectorizer.transform(libmesh_df_merged_d['msg_str'])\n",
    "X_libmesh = hstack((X_libmesh,np.array(libmesh_df_merged_d['buggy_topic'])[:,None]))\n",
    "print(X_libmesh.shape)\n",
    "libmesh_df_merged_d['pred_code'] = clf.predict(X_libmesh)\n",
    "print(libmesh_df_merged_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "y_df = libmesh_df_merged_d[['buggy', 'code', 'pred_code']]\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "y_df = y_df[y_df['code'] != 'undetermined']\n",
    "print(y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "# model_knn.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10\n",
    "# km = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=5, verbose=1)\n",
    "# km.fit(X)\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.hist(km.labels_, bins=k)\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# cluster_assignments_dict = {}\n",
    "# # print(np.where(km.labels_ == i))\n",
    "# # print(raw_df.iloc[79]['msg'])\n",
    "# for i in set(km.labels_):\n",
    "# #     print(i)\n",
    "#     current_cluster_vals = [(raw_df.iloc[x]['msg'], raw_df.iloc[x]['buggy']) for x in np.where(km.labels_ == i)[0]]\n",
    "#     cluster_assignments_dict[i] = current_cluster_vals\n",
    "\n",
    "# cluster_pick = np.random.choice(len(set(km.labels_)))\n",
    "# print('Cluster {0}'.format(cluster_pick))\n",
    "# cluster_assignments_dict[cluster_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
